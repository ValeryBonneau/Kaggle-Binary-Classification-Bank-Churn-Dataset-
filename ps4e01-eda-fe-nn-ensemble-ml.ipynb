{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/valerybonneau/ps4e01-eda-fe-nn-ensemble-ml?scriptVersionId=193455759\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n# Table of content\n0. [Introduction](#zero)\n1. [Libraries and data import](#one)\n2. [EDA](#two)\n3. [Data Preparation](#three)\n4. [Feature Engineering](#four)\n5. [Models Training](#five)\n6. [NN Focus](#six)\n7. [ensemble](#seven)\n8. [Conclusion and Submission](#eight)","metadata":{}},{"cell_type":"markdown","source":"<a id='zero'></a>\n# 0. Introduction\nIt is a very quick and dirty implementation. I'm running AutoMl without the slightest EDA. It will give me a baseline that I'll use for my other notebooks.<br>\nThe first test gives 0.89094 and a temporary eleventh place on the lb.\n\n## A. Problem Scope\nThe goal is to predict if a customer will leave or not his or her bank.The data are taken from that [Bank Customer Churn Dataset](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). \n\n## B. Personal Goal\nI have created a [notebook](https://www.kaggle.com/code/valerybonneau/ps4e01-automl-test-0-89094-lb-score) that includes an AutoML submission and gave a public score of 0.89094. My plan is to focus on data preparation, feature engineering and neural network to improve that score and ideally, end up in the top 15%.\n\n## C. Acknowledgments\nI have been through a lot of notebooks and read or contribute to discussions. Some notebooks or people really did help me a lot (on top of the community). Thanks to... Have a look a their notebooks!\n\nGood luck with the competition!","metadata":{}},{"cell_type":"markdown","source":"<a id='one'></a>\n# 1. Libraries and data import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-29T11:06:09.78019Z","iopub.execute_input":"2024-01-29T11:06:09.780961Z","iopub.status.idle":"2024-01-29T11:06:10.247126Z","shell.execute_reply.started":"2024-01-29T11:06:09.780922Z","shell.execute_reply":"2024-01-29T11:06:10.24566Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e1/sample_submission.csv\n/kaggle/input/playground-series-s4e1/train.csv\n/kaggle/input/playground-series-s4e1/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install scikit-learn==1.3.2","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:10.249804Z","iopub.execute_input":"2024-01-29T11:06:10.250451Z","iopub.status.idle":"2024-01-29T11:06:29.891846Z","shell.execute_reply.started":"2024-01-29T11:06:10.250404Z","shell.execute_reply":"2024-01-29T11:06:29.890803Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.3.2\n  Obtaining dependency information for scikit-learn==1.3.2 from https://files.pythonhosted.org/packages/d0/0b/26ad95cf0b747be967b15fb71a06f5ac67aba0fd2f9cd174de6edefc4674/scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.24.3)\nRequirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2) (3.2.0)\nDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.3.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport time\n\n# math and stats\nimport scipy.stats as stats\nfrom scipy.stats import randint, uniform\nimport tensorflow_probability as tfp\nfrom scipy.stats import boxcox\n\n# Preprocessing\nimport sklearn\nfrom sklearn.model_selection import cross_validate, KFold, RandomizedSearchCV, StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Simple Model\nfrom sklearn.linear_model import LogisticRegression, Ridge, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# ensemble models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Neural network model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\n\n# Models \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score\n\n\n# lOOK AND FEEL\nsns.set(style='darkgrid')\nsns.set_palette(sns.color_palette(\"Set2\"))\n\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\n# Global Variable\nrandom_seed = 73\nepsilon = 1e-7","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:29.893413Z","iopub.execute_input":"2024-01-29T11:06:29.893782Z","iopub.status.idle":"2024-01-29T11:06:46.982107Z","shell.execute_reply.started":"2024-01-29T11:06:29.893746Z","shell.execute_reply":"2024-01-29T11:06:46.980499Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Description\nThe following description is taken from this [notebook](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). <br>The bank customer churn dataset is a commonly used dataset for predicting customer churn in the banking industry. It contains information on bank customers who either left the bank or continue to be a customer. The dataset includes the following attributes:\n\n- **Customer ID**: A unique identifier for each customer\n- **Surname**: The customer's surname or last name\n- **Credit Score**: A numerical value representing the customer's credit score\n- **Geography**: The country where the customer resides (France, Spain or Germany)\n- **Gender**: The customer's gender (Male or Female)\n- **Age**: The customer's age.\n- **Tenure**: The number of years the customer has been with the bank\n- **Balance**: The customer's account balance\n- **NumOfProducts**: The number of bank products the customer uses (e.g., savings account, credit card)\n- **HasCrCard**: Whether the customer has a credit card (1 = yes, 0 = no)\n- **IsActiveMember**: Whether the customer is an active member (1 = yes, 0 = no)\n- **EstimatedSalary**: The estimated salary of the customer\n- **Exited**: Whether the customer has churned (1 = yes, 0 = no)","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s4e1/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e1/test.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:46.984845Z","iopub.execute_input":"2024-01-29T11:06:46.985655Z","iopub.status.idle":"2024-01-29T11:06:47.712146Z","shell.execute_reply.started":"2024-01-29T11:06:46.985618Z","shell.execute_reply":"2024-01-29T11:06:47.710927Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id  CustomerId         Surname  CreditScore Geography Gender   Age  Tenure  \\\n0   0    15674932  Okwudilichukwu          668    France   Male  33.0       3   \n1   1    15749177   Okwudiliolisa          627    France   Male  33.0       1   \n2   2    15694510           Hsueh          678    France   Male  40.0      10   \n3   3    15741417             Kao          581    France   Male  34.0       2   \n4   4    15766172       Chiemenam          716     Spain   Male  33.0       5   \n\n     Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n0       0.00              2        1.0             0.0        181449.97   \n1       0.00              2        1.0             1.0         49503.50   \n2       0.00              2        1.0             0.0        184866.69   \n3  148882.54              1        1.0             1.0         84560.88   \n4       0.00              2        1.0             1.0         15068.83   \n\n   Exited  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>15674932</td>\n      <td>Okwudilichukwu</td>\n      <td>668</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>181449.97</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>15749177</td>\n      <td>Okwudiliolisa</td>\n      <td>627</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>49503.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>15694510</td>\n      <td>Hsueh</td>\n      <td>678</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>184866.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15741417</td>\n      <td>Kao</td>\n      <td>581</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>148882.54</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>84560.88</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>15766172</td>\n      <td>Chiemenam</td>\n      <td>716</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>15068.83</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:47.713524Z","iopub.execute_input":"2024-01-29T11:06:47.714121Z","iopub.status.idle":"2024-01-29T11:06:47.822602Z","shell.execute_reply.started":"2024-01-29T11:06:47.714082Z","shell.execute_reply":"2024-01-29T11:06:47.821594Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                    count          mean           std          min  \\\nid               165034.0  8.251650e+04  47641.356500         0.00   \nCustomerId       165034.0  1.569201e+07  71397.816791  15565701.00   \nCreditScore      165034.0  6.564544e+02     80.103340       350.00   \nAge              165034.0  3.812589e+01      8.867205        18.00   \nTenure           165034.0  5.020353e+00      2.806159         0.00   \nBalance          165034.0  5.547809e+04  62817.663278         0.00   \nNumOfProducts    165034.0  1.554455e+00      0.547154         1.00   \nHasCrCard        165034.0  7.539537e-01      0.430707         0.00   \nIsActiveMember   165034.0  4.977702e-01      0.499997         0.00   \nEstimatedSalary  165034.0  1.125748e+05  50292.865585        11.58   \nExited           165034.0  2.115988e-01      0.408443         0.00   \n\n                         25%         50%           75%          max  \nid                  41258.25     82516.5  1.237748e+05    165033.00  \nCustomerId       15633141.00  15690169.0  1.575682e+07  15815690.00  \nCreditScore           597.00       659.0  7.100000e+02       850.00  \nAge                    32.00        37.0  4.200000e+01        92.00  \nTenure                  3.00         5.0  7.000000e+00        10.00  \nBalance                 0.00         0.0  1.199395e+05    250898.09  \nNumOfProducts           1.00         2.0  2.000000e+00         4.00  \nHasCrCard               1.00         1.0  1.000000e+00         1.00  \nIsActiveMember          0.00         0.0  1.000000e+00         1.00  \nEstimatedSalary     74637.57    117948.0  1.551525e+05    199992.48  \nExited                  0.00         0.0  0.000000e+00         1.00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>165034.0</td>\n      <td>8.251650e+04</td>\n      <td>47641.356500</td>\n      <td>0.00</td>\n      <td>41258.25</td>\n      <td>82516.5</td>\n      <td>1.237748e+05</td>\n      <td>165033.00</td>\n    </tr>\n    <tr>\n      <th>CustomerId</th>\n      <td>165034.0</td>\n      <td>1.569201e+07</td>\n      <td>71397.816791</td>\n      <td>15565701.00</td>\n      <td>15633141.00</td>\n      <td>15690169.0</td>\n      <td>1.575682e+07</td>\n      <td>15815690.00</td>\n    </tr>\n    <tr>\n      <th>CreditScore</th>\n      <td>165034.0</td>\n      <td>6.564544e+02</td>\n      <td>80.103340</td>\n      <td>350.00</td>\n      <td>597.00</td>\n      <td>659.0</td>\n      <td>7.100000e+02</td>\n      <td>850.00</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>165034.0</td>\n      <td>3.812589e+01</td>\n      <td>8.867205</td>\n      <td>18.00</td>\n      <td>32.00</td>\n      <td>37.0</td>\n      <td>4.200000e+01</td>\n      <td>92.00</td>\n    </tr>\n    <tr>\n      <th>Tenure</th>\n      <td>165034.0</td>\n      <td>5.020353e+00</td>\n      <td>2.806159</td>\n      <td>0.00</td>\n      <td>3.00</td>\n      <td>5.0</td>\n      <td>7.000000e+00</td>\n      <td>10.00</td>\n    </tr>\n    <tr>\n      <th>Balance</th>\n      <td>165034.0</td>\n      <td>5.547809e+04</td>\n      <td>62817.663278</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.199395e+05</td>\n      <td>250898.09</td>\n    </tr>\n    <tr>\n      <th>NumOfProducts</th>\n      <td>165034.0</td>\n      <td>1.554455e+00</td>\n      <td>0.547154</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>2.0</td>\n      <td>2.000000e+00</td>\n      <td>4.00</td>\n    </tr>\n    <tr>\n      <th>HasCrCard</th>\n      <td>165034.0</td>\n      <td>7.539537e-01</td>\n      <td>0.430707</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.000000e+00</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>IsActiveMember</th>\n      <td>165034.0</td>\n      <td>4.977702e-01</td>\n      <td>0.499997</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.000000e+00</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>EstimatedSalary</th>\n      <td>165034.0</td>\n      <td>1.125748e+05</td>\n      <td>50292.865585</td>\n      <td>11.58</td>\n      <td>74637.57</td>\n      <td>117948.0</td>\n      <td>1.551525e+05</td>\n      <td>199992.48</td>\n    </tr>\n    <tr>\n      <th>Exited</th>\n      <td>165034.0</td>\n      <td>2.115988e-01</td>\n      <td>0.408443</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:47.824086Z","iopub.execute_input":"2024-01-29T11:06:47.824492Z","iopub.status.idle":"2024-01-29T11:06:47.8783Z","shell.execute_reply.started":"2024-01-29T11:06:47.824462Z","shell.execute_reply":"2024-01-29T11:06:47.877373Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 165034 entries, 0 to 165033\nData columns (total 14 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   id               165034 non-null  int64  \n 1   CustomerId       165034 non-null  int64  \n 2   Surname          165034 non-null  object \n 3   CreditScore      165034 non-null  int64  \n 4   Geography        165034 non-null  object \n 5   Gender           165034 non-null  object \n 6   Age              165034 non-null  float64\n 7   Tenure           165034 non-null  int64  \n 8   Balance          165034 non-null  float64\n 9   NumOfProducts    165034 non-null  int64  \n 10  HasCrCard        165034 non-null  float64\n 11  IsActiveMember   165034 non-null  float64\n 12  EstimatedSalary  165034 non-null  float64\n 13  Exited           165034 non-null  int64  \ndtypes: float64(5), int64(6), object(3)\nmemory usage: 17.6+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'The number of NA for the train set is {train.isna().sum().sum()}')\nprint(f'The number of NA for the test set is {test.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:06:47.879359Z","iopub.execute_input":"2024-01-29T11:06:47.880681Z","iopub.status.idle":"2024-01-29T11:06:47.93265Z","shell.execute_reply.started":"2024-01-29T11:06:47.880614Z","shell.execute_reply":"2024-01-29T11:06:47.931269Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The number of NA for the train set is 0\nThe number of NA for the test set is 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='two'></a>\n# 2. EDA","metadata":{}},{"cell_type":"code","source":"customers = train.copy().drop(columns=['id', 'Surname'])\nfeatures = customers.columns.tolist()\ntarget = train[features.pop()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_num = ['CustomerId', 'CreditScore', 'Age', 'Balance', 'EstimatedSalary']\nfeat_cat = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution\nNot too sure about what needs to be done here. `CreditScore` has a normal distribution, `Age` is a little bit right skewed but if I remove some outliers, that should  be enough. `Balance` is more problematic with its high number of zeros. Also, I guess I should remove `CustomerId` but it could contain indirect information about the tenure.","metadata":{}},{"cell_type":"code","source":"_, axes = plt.subplots(nrows=2, ncols=3, figsize=(18,12))\nplt.suptitle('Distribution of numerical features')\nfor feat,ax in zip(feat_num, axes.ravel()):\n    sns.violinplot(data=customers, x='Exited', y=feat, ax=ax)\naxes[-1,-1].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,12))\nfor feat, ax in zip(feat_cat, axes.ravel()):\n    sns.countplot(data=customers, x=feat, ax=ax, hue='Exited')\n    ax.set_title(f'{feat} vs Exited')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"_, axes = plt.subplots(nrows=2, ncols=3, figsize=(12,10))\nplt.suptitle('Outliers detection')\nfor feat, ax in zip(feat_num, axes.ravel()):\n    sns.boxplot(data=customers, x='Exited', y=feat, ax=ax)\n    ax.set_xlabel('')\naxes[-1,-1].set_visible(False)\nplt.tight_layout()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix\nI've rarely seen dataset with such a low correlation between features and target. At first glance, it's like no feature has any impact on the target. The feature engineering might be challenging.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,10)})\n\ncol_not_object = customers.select_dtypes(exclude='object')\ncustomers_corr = customers[col_not_object.columns]\ncorr = customers_corr.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(100, 7, s = 75, l = 40, n = 20, center = 'light', as_cmap = True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={'shrink':.5}, annot=True, fmt='.1f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.pairplot(customers.sample(frac=.02),\n                 hue='Exited',\n                 corner=True\n                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.countplot(data=customers, x='Exited')\nclass_weight = (customers['Exited'].value_counts(normalize=True)*100).round(2).tolist()\nfor idx, value in enumerate(class_weight):\n    plt.text(idx, 10000, f'{value:.2f}%', color='white', ha='center', fontweight='bold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom ydata_profiling import ProfileReport\n    \n# Generate the profile report\nprofile = ProfileReport(train, title='PS4E01 - Automated Report')\nprofile.to_notebook_iframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.step(time, survival_prob, where='post')\n# plt.xlabel('Tenure (years)')\n# plt.ylabel('Probability to Exit (%)')\n# plt.suptitle('Probability to Exit with Tenure evolution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='three'></a>\n# 3. Data Preparation","metadata":{}},{"cell_type":"code","source":"def normalize_features(X1, X2, feat_tr):\n    \"\"\"\n    Normalize features from X1 and X2 using the appropriate transformation \n            \n    Args:\n        X1 (dataframe): dataframe corresponding to X_train\n        X2 (dataframe): dataframe corresponding to X_test\n        feat_tr: list of features to treat.\n        \n    Returns:\n        X1, X2: the dataframes after normalization of their features.\n    \"\"\"\n    X1 = X1.copy()\n    X2 = X2.copy()\n    \n    # I treat only right skewed for now\n    for feat in feat_tr:\n        X1[feat] = boxcox(X1[feat]+epsilon, lmbda=0.05)\n        X2[feat] = boxcox(X2[feat]+epsilon, lmbda=0.05)\n            \n    return X1, X2","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:07:33.124283Z","iopub.execute_input":"2024-01-29T11:07:33.124886Z","iopub.status.idle":"2024-01-29T11:07:33.134365Z","shell.execute_reply.started":"2024-01-29T11:07:33.12484Z","shell.execute_reply":"2024-01-29T11:07:33.132737Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def scale_features(X1, X2=pd.DataFrame()):\n    \"\"\"\n    Scale X1 and X2. \n    Apply fit_transform to X1 and transform to X2 and returns the result\n    \n    Args:\n        X1 (dataframe): dataframe corresponding to X_train\n        X2 (dataframe): dataframe corresponding to X_test\n        \n    Returns:\n        X1, X2: the dataframes after scaling.\n    \"\"\"\n    \n    X1 = X1.copy()\n    X2 = X2.copy()\n    \n    cols = X1.columns\n    scaler= MinMaxScaler()\n    \n    X1 = scaler.fit_transform(X1)\n    X2 = scaler.transform(X2)\n\n    X1 = pd.DataFrame(data=X1, columns=cols)\n    X2 = pd.DataFrame(data=X2, columns=cols)\n    \n    return X1, X2","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:10:17.941748Z","iopub.execute_input":"2024-01-29T11:10:17.942155Z","iopub.status.idle":"2024-01-29T11:10:17.949849Z","shell.execute_reply.started":"2024-01-29T11:10:17.942122Z","shell.execute_reply":"2024-01-29T11:10:17.94866Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## X, y creation","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(columns=['id', 'CustomerId', 'Exited'])\nX_test = test.drop(columns=['id', 'CustomerId'])\ny_train = train['Exited']","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:09.854465Z","iopub.execute_input":"2024-01-29T11:14:09.854947Z","iopub.status.idle":"2024-01-29T11:14:09.873856Z","shell.execute_reply.started":"2024-01-29T11:14:09.854909Z","shell.execute_reply":"2024-01-29T11:14:09.872592Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"markdown","source":"## Encoding\nInitially, I thought there was only `Geography` and `Gender` to encode. I use one-hot encoding for `Geography`.\nNow, I will make a last try with `Surname` and use binary encoding.","metadata":{}},{"cell_type":"code","source":"X_train = pd.get_dummies(X_train, columns=['Geography'], prefix=['Country'], drop_first=True)\nX_test = pd.get_dummies(X_test, columns=['Geography'], prefix=['Country'], drop_first=True)\n\nX_train['Gender'] = X_train['Gender'].map({'Male':0, 'Female':1}).astype('bool')\nX_test['Gender'] = X_test['Gender'].map({'Male':0, 'Female':1}).astype('bool')\n\nfrom category_encoders import BinaryEncoder\nbe = BinaryEncoder(cols=['Surname'])\ntrain_surname = be.fit_transform(X_train['Surname'])\ntest_surname = be.transform(X_test['Surname'])\n\nX_train = pd.concat([X_train, train_surname], axis=1 ) \nX_test = pd.concat([X_test, test_surname], axis=1)\n\nX_train = X_train.drop(columns=['Surname'])\nX_test = X_test.drop(columns=['Surname'])","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:11.161522Z","iopub.execute_input":"2024-01-29T11:14:11.161968Z","iopub.status.idle":"2024-01-29T11:14:11.514254Z","shell.execute_reply.started":"2024-01-29T11:14:11.161933Z","shell.execute_reply":"2024-01-29T11:14:11.513101Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"feat_tr = ['Age', 'CreditScore', 'Balance', 'EstimatedSalary']","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:14.45705Z","iopub.execute_input":"2024-01-29T11:14:14.457439Z","iopub.status.idle":"2024-01-29T11:14:14.462561Z","shell.execute_reply.started":"2024-01-29T11:14:14.457409Z","shell.execute_reply":"2024-01-29T11:14:14.461561Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"<a id='four'><a/>\n# 4. Feature Engineering\nUnder construction","metadata":{}},{"cell_type":"code","source":"def add_feature(X1, feat_list):\n    \"\"\"\n    Add new features to X1 for each feature contained in the feat_list\n    \n    Args:\n        X1 (dataframe): the dataframe to remove outliers from\n        feat_list: a list that contains all the feature to work on\n         \n    Returns:\n        X1: the dataframes with the new features\n    \"\"\"\n    epsilon = 1e-7\n    X = X1.copy()\n    for feat in feat_list:\n        X[str(feat) + '_square'] = np.square(X[feat])\n#         X[str(feat) + '_cube'] = np.power(X[feat],3)\n        X[str(feat) + '_log'] = np.log1p(X[feat] + epsilon)\n        \n    return X","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:25.857222Z","iopub.execute_input":"2024-01-29T11:14:25.85766Z","iopub.status.idle":"2024-01-29T11:14:25.864156Z","shell.execute_reply.started":"2024-01-29T11:14:25.857627Z","shell.execute_reply":"2024-01-29T11:14:25.862808Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from itertools import combinations, product\n\ndef add_combination_feature(X, feat_list):\n    \"\"\"\n    Add new features to X1 for each feature contained in the feat_list based on the other feature of X1\n    \n    Args:\n        X1 (dataframe): the dataframe to remove outliers from\n        feat_list: a list that contains all the feature to work on\n         \n    Returns:\n        X1: the dataframes with the new features\n    \"\"\"\n    epsilon = 1e-7\n    new_columns = []\n    \n    X1 = X.copy()\n\n    for feat1, feat2 in product(feat_list, repeat=2):\n        if feat1 != feat2:\n            new_col_name_div = f'{feat1}_div_{feat2}'\n            X1[new_col_name_div] = X1[feat1] / (X1[feat2]+epsilon)\n            \n            new_col_name_times = f'{feat1}_times_{feat2}'\n            X1[new_col_name_times] = X1[feat1] * X1[feat2]\n            \n    return X1","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:27.103247Z","iopub.execute_input":"2024-01-29T11:14:27.103662Z","iopub.status.idle":"2024-01-29T11:14:27.1128Z","shell.execute_reply.started":"2024-01-29T11:14:27.103631Z","shell.execute_reply":"2024-01-29T11:14:27.111238Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Adapted from the 'feature engineering' kaggle course: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, random_state=random_seed)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:28.771996Z","iopub.execute_input":"2024-01-29T11:14:28.772421Z","iopub.status.idle":"2024-01-29T11:14:28.778503Z","shell.execute_reply.started":"2024-01-29T11:14:28.772386Z","shell.execute_reply":"2024-01-29T11:14:28.777291Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"features = X_train.columns\nX_train_fi, X_test_fi = scale_features(X_train, X_test)\n\nmi_scores0 = make_mi_scores(X_train_fi, y_train, features)\nmi_scores0","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:14:33.167401Z","iopub.execute_input":"2024-01-29T11:14:33.167856Z","iopub.status.idle":"2024-01-29T11:15:07.593898Z","shell.execute_reply.started":"2024-01-29T11:14:33.16782Z","shell.execute_reply":"2024-01-29T11:15:07.592804Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"NumOfProducts      0.089390\nAge                0.082859\nIsActiveMember     0.023768\nCountry_Germany    0.017628\nBalance            0.015483\nGender             0.012631\nEstimatedSalary    0.008355\nCreditScore        0.004411\nSurname_5          0.003197\nSurname_1          0.003143\nSurname_3          0.002395\nCountry_Spain      0.002226\nTenure             0.001383\nSurname_2          0.000951\nSurname_7          0.000810\nSurname_6          0.000742\nSurname_10         0.000631\nSurname_0          0.000576\nHasCrCard          0.000343\nSurname_9          0.000049\nSurname_4          0.000000\nSurname_8          0.000000\nSurname_11         0.000000\nName: MI Scores, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"mi_scores0.to_csv('mi_scores0.csv', index=True, sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:05:25.535146Z","iopub.execute_input":"2024-01-27T18:05:25.535562Z","iopub.status.idle":"2024-01-27T18:05:25.54429Z","shell.execute_reply.started":"2024-01-27T18:05:25.53553Z","shell.execute_reply":"2024-01-27T18:05:25.542862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_l = ['Age', 'CreditScore', 'Tenure', 'Balance', 'EstimatedSalary', 'NumOfProducts']\n\nX_train_f = add_combination_feature(X_train, feat_l)\nX_test_f = add_combination_feature(X_test, feat_l)\n\nX_train_c = add_feature(X_train_f, feat_l)\nX_test_c = add_feature(X_test_f, feat_l)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:29:45.115997Z","iopub.execute_input":"2024-01-27T19:29:45.11662Z","iopub.status.idle":"2024-01-27T19:29:45.518604Z","shell.execute_reply.started":"2024-01-27T19:29:45.116577Z","shell.execute_reply":"2024-01-27T19:29:45.517185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = X_train_c.columns\nX_train_fi, X_test_fi = scale_features(X_train_c, X_test_c)\n\nmi_scores1 = make_mi_scores(X_train_fi, y_train, features)\nmi_scores1.to_csv('mi_scores1.csv', index=True, sep=',')\nmi_scores1","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:30:13.028077Z","iopub.execute_input":"2024-01-27T19:30:13.028556Z","iopub.status.idle":"2024-01-27T19:32:36.52886Z","shell.execute_reply.started":"2024-01-27T19:30:13.028517Z","shell.execute_reply":"2024-01-27T19:32:36.527373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_f = add_combination_feature(X_train_c, feat_l)\nX_test_f = add_combination_feature(X_test_c, feat_l)\n\nX_train_f.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = X_train.columns\nX_train_fi, X_test_fi = scale_features(X_train_f, X_test_f)\n\nmi_scores = make_mi_scores(X_train_fi, y_train, features)\nmi_scores.to_csv('mi_scores.csv', index=True, sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T17:51:38.244034Z","iopub.execute_input":"2024-01-27T17:51:38.244798Z","iopub.status.idle":"2024-01-27T18:03:58.253526Z","shell.execute_reply.started":"2024-01-27T17:51:38.244731Z","shell.execute_reply":"2024-01-27T18:03:58.252158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_best_scores = mi_scores[:50].index","metadata":{"execution":{"iopub.status.busy":"2024-01-27T17:23:49.474283Z","iopub.execute_input":"2024-01-27T17:23:49.475735Z","iopub.status.idle":"2024-01-27T17:23:49.483092Z","shell.execute_reply.started":"2024-01-27T17:23:49.475681Z","shell.execute_reply":"2024-01-27T17:23:49.481452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP analysis\nI'm not very fgood at interpreting feature importance or feature contribution so I'll spend a bit of time on SHAP.","metadata":{}},{"cell_type":"code","source":"import shap\n\n# I'm using the parameters and the model that best performed so far with a score of 0.8902554795195133\nparams_shap = {'n_estimators': 1464,\n                      'max_depth': 41,\n                      'learning_rate': 0.03433,\n                      'min_child_weight': 2.96035,\n                      'min_child_samples': 30,\n                      'subsample': 0.878299,\n                      'subsample_freq': 3,\n                      'colsample_bytree': 0.501275,\n                      'num_leaves': 25}\nmodel_shap = LGBMClassifier(random_state=random_seed,verbose=-1, **params_shap).fit(X_train, y_train)\nexplainer = shap.TreeExplainer(model_shap)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:18:56.876334Z","iopub.execute_input":"2024-01-27T18:18:56.876915Z","iopub.status.idle":"2024-01-27T18:19:22.723131Z","shell.execute_reply.started":"2024-01-27T18:18:56.876856Z","shell.execute_reply":"2024-01-27T18:19:22.721915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sampled = X_train.sample(1000, random_state=73)\nshap_values = explainer.shap_values(X_sampled)\nshap.summary_plot(shap_values[1], X_sampled)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:33:58.872092Z","iopub.execute_input":"2024-01-27T18:33:58.872864Z","iopub.status.idle":"2024-01-27T18:34:02.372134Z","shell.execute_reply.started":"2024-01-27T18:33:58.87281Z","shell.execute_reply":"2024-01-27T18:34:02.370904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:34:07.982051Z","iopub.execute_input":"2024-01-27T18:34:07.982519Z","iopub.status.idle":"2024-01-27T18:34:07.996471Z","shell.execute_reply.started":"2024-01-27T18:34:07.982479Z","shell.execute_reply":"2024-01-27T18:34:07.995321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1], X_sampled)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:34:19.961693Z","iopub.execute_input":"2024-01-27T18:34:19.962145Z","iopub.status.idle":"2024-01-27T18:34:26.463844Z","shell.execute_reply.started":"2024-01-27T18:34:19.962108Z","shell.execute_reply":"2024-01-27T18:34:26.462348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_shap2 = LGBMClassifier(random_state=random_seed,verbose=-1, **params_shap).fit(X_train_c, y_train)\nexplainer = shap.TreeExplainer(model_shap2)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:55:59.221774Z","iopub.execute_input":"2024-01-27T18:55:59.222578Z","iopub.status.idle":"2024-01-27T18:56:29.410464Z","shell.execute_reply.started":"2024-01-27T18:55:59.222533Z","shell.execute_reply":"2024-01-27T18:56:29.409134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sampled = X_train_c.sample(1000, random_state=73)\nshap_values = explainer.shap_values(X_sampled)\nshap.summary_plot(shap_values[1], X_sampled)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:51:22.189772Z","iopub.execute_input":"2024-01-27T18:51:22.19025Z","iopub.status.idle":"2024-01-27T18:51:26.706749Z","shell.execute_reply.started":"2024-01-27T18:51:22.190215Z","shell.execute_reply":"2024-01-27T18:51:26.705818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_shap3 = LGBMClassifier(random_state=random_seed,verbose=-1, **params_shap).fit(X_train_f, y_train)\nexplainer = shap.TreeExplainer(model_shap3)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:56:04.347883Z","iopub.execute_input":"2024-01-27T19:56:04.348552Z","iopub.status.idle":"2024-01-27T19:56:51.910773Z","shell.execute_reply.started":"2024-01-27T19:56:04.348478Z","shell.execute_reply":"2024-01-27T19:56:51.909276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sampled = X_train_f.sample(1000, random_state=73)\nshap_values = explainer.shap_values(X_sampled)\nshap.summary_plot(shap_values[1], X_sampled)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:56:51.912902Z","iopub.execute_input":"2024-01-27T19:56:51.913426Z","iopub.status.idle":"2024-01-27T19:56:57.979105Z","shell.execute_reply.started":"2024-01-27T19:56:51.913386Z","shell.execute_reply":"2024-01-27T19:56:57.977761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_c.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:58:02.75019Z","iopub.execute_input":"2024-01-27T19:58:02.750681Z","iopub.status.idle":"2024-01-27T19:58:02.760221Z","shell.execute_reply.started":"2024-01-27T19:58:02.750641Z","shell.execute_reply":"2024-01-27T19:58:02.758901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='five'><a/>\n# 5. Model Training\nUnder construction","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n# adapted from https://www.kaggle.com/code/ambrosm/pss3e23-eda-which-makes-sense\n# Thanks a lot to AmbrosM (https://www.kaggle.com/ambrosm)\n\ndef cross_val(model, X, y):\n    tic = time.time()\n       \n    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n    \n    roc_auc_score_te = []\n    roc_auc_score_tr = []\n\n    for fold, (indX_tr, indX_va) in enumerate(kf.split(X, y)):\n        X_tr = X.iloc[indX_tr]\n        X_va = X.iloc[indX_va]\n        y_tr = y.iloc[indX_tr]\n        y_va = y.iloc[indX_va]\n        \n        X_tr, X_va = scale_features(X_tr, X_va)\n\n        model.fit(X_tr, y_tr)\n        y_va_pred = model.predict_proba(X_va)\n        y_tr_pred = model.predict_proba(X_tr)\n        \n        roc_auc_score_te.append(roc_auc_score(y_va, y_va_pred[:, 1]))\n        roc_auc_score_tr.append(roc_auc_score(y_tr, y_tr_pred[:, 1]))\n        \n    roc_auc_score_te = np.array(roc_auc_score_te).mean()\n    roc_auc_score_tr = np.array(roc_auc_score_tr).mean()\n    \n    tac= time.time()\n    print(f'training score: {roc_auc_score_tr}')\n    print(f'execution time of {model}: {round((tac-tic),2)} seconds')\n    return roc_auc_score_tr, roc_auc_score_te","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:59:50.541075Z","iopub.execute_input":"2024-01-27T19:59:50.541583Z","iopub.status.idle":"2024-01-27T19:59:50.553436Z","shell.execute_reply.started":"2024-01-27T19:59:50.541544Z","shell.execute_reply":"2024-01-27T19:59:50.55209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following parameters are the result of Optima test that I did on another [notebook](https://www.kaggle.com/code/valerybonneau/ps4e01-optuna). There also few configurations that I found on notebook from others, unfortunately, I did not write down every contribution. Shame on me.","metadata":{}},{"cell_type":"code","source":"best_params_cbc0= {'learning_rate': 0.010119738958122336,\n                          'iterations': 56,\n                          'depth': 3,\n                          'min_child_samples': 18,\n                          'subsample': 0.8025865440158371}\ncbc0 = CatBoostClassifier(**best_params_cbc0)\n\n\nbest_params_cbc1= {'learning_rate': 0.04328357318741085,\n                   'iterations': 691,\n                   'depth': 5,\n                   'min_child_samples': 39,\n                   'subsample': 0.6809809689623991,\n                   'colsample_bylevel': 0.6243594916266789,\n                   'scale_pos_weight': 7.671793504598362,\n                   'reg_lambda': 8.526571843521985}\ncbc1 = CatBoostClassifier(**best_params_cbc1)\n\ncbc2 = CatBoostClassifier(iterations=7500,od_type=\"Iter\",eval_metric=\"AUC\", random_seed=random_seed,early_stopping_rounds=2000)\n\nbest_params_xgbc0= {'learning_rate': 0.010003408261642718,\n                           'n_estimators': 50,\n                           'max_depth': 3,\n                           'min_child_weight': 1.3295484808253735,\n                           'subsample': 0.5330192943323063,\n                           'colsample_bytree': 0.9840324890323718}\nxgbc0 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc0)\n\nbest_params_xgbc1 = {'alpha': 0.21924507998693532,\n   'colsample_bytree': 0.6190053199553928,\n   'gamma': 0.20385269148738444,\n   'lambda': 0.0007476286271033139,\n   'learning_rate': 0.03361534115324951,\n   'max_depth': 6,\n   'min_child_weight': 5,\n   'n_estimators': 427,\n   'subsample': 0.607646482122379}\nxgbc1 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc1)\n\nbest_params_xgbc2 = {\n   'colsample_bytree': 0.5559356744724407,\n   'gamma': 0.20385269148738444,\n   'lambda': 0.0007476286271033139,\n   'learning_rate': 0.14557589845509303,\n   'max_depth': 4,\n   'min_child_weight': 8.863137333171029,\n   'n_estimators': 108}\nxgbc2 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc2)\n\n\nbest_params_xgbc3 = {\n    'n_estimators': 474,\n    'learning_rate': 0.121800,\n    'max_depth': 3,\n    'num_leaves': 14,\n    'min_child_samples': 2,\n    'subsample':0.882688,\n    'colsample_bytree': 0.822104,\n    'reg_alpha':0.870367,\n    'reg_lambda': 0.855034}\nxgbc3 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc3)\n\nbest_params_xgbc4= {\n    'tree_method': 'hist',\n    'eval_metric': 'auc',\n    'colsample_bytree': 0.40,\n    'learning_rate': 0.06,\n    'max_depth': 9,\n    'n_estimators': 2500,                         \n    'reg_alpha': 0.12,\n    'reg_lambda': 0.8,\n    'min_child_weight': 15,\n}\nxgbc4 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc4)\n\n\nbest_params_xgbc5= {\n    'tree_method': 'hist',\n    'eval_metric': 'auc',\n    'colsample_bytree': 0.5,\n    'learning_rate': 0.055,\n    'max_depth': 9,\n    'n_estimators': 3000,\n    'reg_alpha': 0.2,\n    'reg_lambda': 0.6,\n    'min_child_weight': 25,\n}\nxgbc5 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc5)\n\n\nbest_params_xgbc6= {\n    'tree_method': 'hist',\n    'eval_metric': 'auc',\n    'colsample_bytree': 0.80,\n    'learning_rate': 0.082,\n    'max_depth': 7,\n    'n_estimators': 2000,\n    'reg_alpha': 0.005,\n    'reg_lambda': 0.95,\n    'min_child_weight': 26,\n}\nxgbc6 = xgb.XGBClassifier(objective='binary:logistic', random_state=random_seed, **best_params_xgbc6)\n\nbest_params_xgbc7 = {\n    'n_estimators': 2000,\n    'max_depth': 6,\n    'min_child_weight': 2.7527,\n    'learning_rate': 0.01527,\n    'subsample': 0.7109,\n    'gamma': 0.2940,\n    'colsample_bytree': 0.50157,\n    'colsample_bylevel': 0.6877,\n    'colsample_bynode': 0.94499}\nxgbc7 = XGBClassifier(random_state=random_seed, **best_params_xgbc7)\n\nbest_params_lgbmc0 = {'n_estimators': 149,\n                     'learning_rate': 0.06172,\n                     'max_depth': 8,\n                     'num_leaves': 32,\n                     'min_child_samples': 7,\n                     'subsample': 0.93605,\n                     'colsample_bytree': 0.85325,\n                     'reg_alpha': 0.567685, \n                     'reg_lambda': 0.913222}\nlgbmc0 = LGBMClassifier(**best_params_lgbmc0, n_jobs=-1)\n\n\nbest_params_lgbmc1 = {'n_estimators': 327,\n                      'learning_rate': 0.096359,\n                      'max_depth': 6,\n                      'num_leaves': 19,\n                      'min_child_samples': 4,\n                      'subsample': 0.676477,\n                      'colsample_bytree': 0.76783,\n                      'reg_alpha': 0.869556,\n                      'reg_lambda': 0.6166327}\nlgbmc1 = LGBMClassifier(**best_params_lgbmc1, n_jobs=-1)\n\nbest_params_lgbmc2 = {'learning_rate': 0.043283,\n                      'iterations': 691,\n                      'depth': 5,\n                      'min_child_samples': 39,\n                      'subsample': 0.680981,\n                      'colsample_bytree': 0.6243591,\n                      'scale_pos_weight': 7.671794,\n                      'reg_lambda': 8.526572}\n\nlgbmc2 = LGBMClassifier(**best_params_lgbmc2, n_jobs=-1)\n\n\nbest_params_lgbmc3 = {'n_estimators': 474,\n                      'learning_rate': 0.12180036837974936,\n                      'max_depth': 3,\n                      'num_leaves': 14,\n                      'min_child_samples': 2,\n                      'subsample': 0.8826885927351279,\n                      'colsample_bytree': 0.8221044666084394,\n                      'reg_alpha': 0.8703670339049474,\n                      'reg_lambda': 0.8550341657145066}\n\nlgbmc3 = LGBMClassifier(**best_params_lgbmc3, n_jobs=-1)\n\n\nbest_params_lgbmc4 = {'n_estimators': 1464,\n                      'max_depth': 41,\n                      'learning_rate': 0.03433,\n                      'min_child_weight': 2.96035,\n                      'min_child_samples': 30,\n                      'subsample': 0.878299,\n                      'subsample_freq': 3,\n                      'colsample_bytree': 0.501275,\n                      'num_leaves': 25}\nlgbmc4 = LGBMClassifier(random_state=random_seed,verbose=-1, **best_params_lgbmc4)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:59:51.971477Z","iopub.execute_input":"2024-01-27T19:59:51.971996Z","iopub.status.idle":"2024-01-27T19:59:52.004043Z","shell.execute_reply.started":"2024-01-27T19:59:51.971953Z","shell.execute_reply":"2024-01-27T19:59:52.002871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = {\n    'cbc0': cbc0,\n    'cbc1': cbc1,\n    'cbc2': cbc2,\n    'xgbc0': xgbc0,    \n    'xgbc1': xgbc1,\n    'xgbc2': xgbc2,\n    'xgbc3': xgbc3,\n    'xgbc4': xgbc4,    \n    'xgbc5': xgbc5,\n    'xgbc6': xgbc6,\n    'xgbc7': xgbc7,\n    'lgbmc0': lgbmc0,\n    'lgbmc1': lgbmc1,\n    'lgbmc2': lgbmc2,\n    'lgbmc3': lgbmc3,\n    'lgbmc4':lgbmc4\n}\n\nscores_model_bp = []\nfor model_name, model_instance in model_names.items():\n    score_train, score_test = cross_val(model_instance, X_train_c, y_train)\n    scores_model_bp.append((model_name, score_train, score_test))\n    print(model_name, score_train, score_test)\nscores_model_bp","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:59:52.740473Z","iopub.execute_input":"2024-01-27T19:59:52.740909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='six'><a/>\n# 6. Neural Network Focus\nUnder construction","metadata":{}},{"cell_type":"code","source":"X_train[['Gender', 'Country_Germany', 'Country_Spain']] = X_train[['Gender', 'Country_Germany', 'Country_Spain']].astype('int32')\nX_test[['Gender', 'Country_Germany', 'Country_Spain']] = X_test[['Gender', 'Country_Germany', 'Country_Spain']].astype('int32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.initializers import Constant\n\nneurons = 64\n\nlrelu = lambda x: keras.activations.relu(x, alpha=0.1)\nleaky_relu = keras.layers.LeakyReLU(alpha=0.3)\n\nmodel = keras.Sequential(\n    [\n        keras.layers.BatchNormalization(input_shape=[X_train.shape[1]], ),\n        keras.layers.Dense(neurons, kernel_initializer='glorot_uniform', activation=None),\n#         keras.layers.BatchNormalization(),\n        keras.layers.Activation('swish'),\n        keras.layers.Dropout(rate=0.1),\n        keras.layers.Dense(neurons, kernel_initializer='glorot_uniform', activation=None),\n        keras.layers.BatchNormalization(),\n        keras.layers.Activation('swish'),\n        keras.layers.Dropout(rate=0.1),\n        keras.layers.Dense(neurons, kernel_initializer='glorot_uniform', activation=None),\n        keras.layers.BatchNormalization(),\n        keras.layers.Activation('swish'),\n        keras.layers.Dropout(rate=0.1),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = (train['Exited'].value_counts(normalize=True)*100).round(2).tolist()\nclass_weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/hridaym25/tensorflow-nn-with-lgbm-weights\n\nclass_weight_dic = {0: class_weight[0]/100, 1: class_weight[1]/100}\n\nbinary_accuracy_metric = tf.keras.metrics.BinaryAccuracy()\nadam_optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n# clipvalue=1.0\n\ndef loss_fn(y_true, y_pred):\n    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n\ndef metric_fn(y_true, y_pred):\n    return binary_accuracy_metric(y_true, y_pred)\n\nmodel.compile(\n    optimizer=adam_optimizer,\n    loss=loss_fn,\n    metrics=[metric_fn],\n)\n\ncallbacks_list = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', \n        patience=100, \n        restore_best_weights=True),\n    tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\", histogram_freq=1, profile_batch = 100000000)\n]\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    class_weight=class_weight_dic,\n    callbacks=callbacks_list,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=35,\n    verbose=2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['metric_fn'])\nplt.plot(history.history['val_metric_fn'], label='val')\nplt.title('AUC score')\nplt.xlabel('epoch')\nplt.ylabel('Auc')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='val')\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='seven'></a>\n# 7. Ensemble Model\nI use hyper parameters that I've identified in that [notebook](https://www.kaggle.com/code/valerybonneau/ps4e01-optuna) using Optuna. Regarding the model that I try to optimize, I picked the one that got the best score with Autogluon and Bluecast. You can find the notebook [here](https://www.kaggle.com/code/valerybonneau/ps4e01-automl-test-with-bluecast-and-autogluon).","metadata":{}},{"cell_type":"code","source":"sorted_score = sorted(scores_model_bp, key=lambda x: x[2], reverse=True)\nbest_score = sorted_score[:6]\nbest_estimators = [(model, model_names[model]) for model, _, _ in best_score]\nbest_score","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:57:04.797115Z","iopub.execute_input":"2024-01-27T18:57:04.797614Z","iopub.status.idle":"2024-01-27T18:57:04.862151Z","shell.execute_reply.started":"2024-01-27T18:57:04.797576Z","shell.execute_reply":"2024-01-27T18:57:04.860432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train_f[features_best_scores]\nX_test = X_test_f[features_best_scores]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T11:32:33.539344Z","iopub.execute_input":"2024-01-27T11:32:33.539809Z","iopub.status.idle":"2024-01-27T11:32:34.074706Z","shell.execute_reply.started":"2024-01-27T11:32:33.539772Z","shell.execute_reply":"2024-01-27T11:32:34.073497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fi, X_test_fi = scale_features(X_train_c, X_test_c)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:57:28.465233Z","iopub.execute_input":"2024-01-27T18:57:28.465721Z","iopub.status.idle":"2024-01-27T18:57:29.01598Z","shell.execute_reply.started":"2024-01-27T18:57:28.465681Z","shell.execute_reply":"2024-01-27T18:57:29.014707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nestimators =( \n    best_estimators\n    )\n\nvr = VotingClassifier(estimators=estimators,\n                     voting='soft',\n                     verbose=False,\n                      weights=[0.22, 0.20, 0.19, .16, 0.13, 0.10],\n                     n_jobs=-1)\nvr.fit(X_train_fi, y_train)\npredictions = vr.predict_proba(X_test_fi)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:33:12.890992Z","iopub.execute_input":"2024-01-27T19:33:12.891448Z","iopub.status.idle":"2024-01-27T19:40:06.846952Z","shell.execute_reply.started":"2024-01-27T19:33:12.89141Z","shell.execute_reply":"2024-01-27T19:40:06.845604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='eight'></a>\n# 8. Submission and Conclusion\nI ended up in the Top 15% as I wanted, learned quite a lot and applied different methods about feature importance.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/playground-series-s4e3/sample_submission.csv\")\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:48:41.188362Z","iopub.execute_input":"2024-01-27T19:48:41.188907Z","iopub.status.idle":"2024-01-27T19:48:41.709074Z","shell.execute_reply.started":"2024-01-27T19:48:41.188867Z","shell.execute_reply":"2024-01-27T19:48:41.70778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}